---
title: "demographics"
author: "Decision Trees and Random Forest"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(ggplot2)
data(Boston)
str(Boston)
```

```{r}
Boston %>% 
  ggplot() +
  geom_histogram(aes(medv,after_stat(density)),bins=10)+
  geom_density(aes(medv))
```  
#create a basic Decision Tree regression model, using rpart
```{r}
library(rpart)
set.seed(100)
model = rpart(medv ~ ., data = Boston, )
model

varImp(model)
```
#model a ridge regression using the Caret package
#Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. 
#pass the same parameters as above, but in addition we pass the method = 'rpart2' model to tell caret to use a lasso model.
#The Least Absolute Shrinkage and Selection Operator (LASSO) is a regularization method that can set the coefficients of some features to zero.
```{r}
library(caret)
set.seed(1)

model <- train(
  medv ~ .,
  data = Boston,
  method = 'rpart2'
)

# caret automatically trained over multiple hyper parameters
model

#trained over multiple hyper parameters
plot(model)
```

#preprocess
```{r}
set.seed(1)

inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]
```

```{r}
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'rpart2',
  preProcess = c("center", "scale")
)
model3
```

#check our data on the test set.
#use the subset method to get the features and test target. 
#then use the predict method passing in our model from above and the test features.
#calculate the RMSE and r2 to compare to the model above.
#use the subset method to get the features and test target
```{r}
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)

# RMSE
sqrt(mean((test.target - predictions)^2))

# R2
cor(test.target, predictions) ^ 2
```

#cross validation
#k-fold cross-validation that resamples and splits our data many times.
#then train the model on these samples and pick the best model. 
#Caret uses trainControl method to make this easy.

#10-fold cross-validation, pass method = "cv", number = 10 
```{r}
set.seed(1)
ctrl <- trainControl(
  method = "cv",
  number = 10,
 )

#retrain our model and pass the ctrl response to the trControl parameter.
set.seed(1)
model4 <- train(
  medv ~ .,
  data = training,
  method = 'rpart2',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4
#The final value used for the model was maxdepth = 3

plot(model4)
#results seemed to have improved our accuracy for our training data
```

```{r}
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model4, newdata = test.features)

# RMSE
sqrt(mean((test.target - predictions)^2))

# R2
cor(test.target, predictions) ^ 2
```

#tune hyperparameters
#ridge model, we can give the model different values of lambda.
```{r}
set.seed(1)

tuneGrid <- expand.grid(
  degree = 1, 
  nprune = c(2, 11, 10)
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'earth',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)

model5
plot(model5)

#plot the model to see how it performs over different tuning parameters.
```

```{r}
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model5, newdata = test.features)

# RMSE
sqrt(mean((test.target - predictions)^2))

# R2
cor(test.target, predictions) ^ 2
```

#Random Forest - tree model that uses the bagging technique.
#Many trees are built up in parallel and used to build a single tree model
#Bagging corresponds to a particular case of random forest when all the regressors are used. For this reason to implement bagging we use the function randomForest in the randomForest package. As the procedure is based on boostrap resampling we need to set the seed. As usual we specify the formula and the data together with the number of regressors to be used (mtry, in this case equal to the number of columns minus the column dedicated to the response variable). The option importance is set to T when we are interested in assessing the importance of the predictors. Finally, ntree represents the number of independent trees to grow (500 is the default value).
```{r}
library(randomForest)
rfNews()
#rfNews() to see new features/changes/bug fixes.
```

#turn target to factor
```{r}
str(Boston)
```

#medv is numeric - regression tree, RMSE
```{r}
set.seed(123)
model = randomForest(medv ~ ., data = Boston, importance=T, ntree=250)
model
```

```{r}
model$mse[500] 

#As before, we compute the predictions and the test MSE:
# Generate predictions on a test set
tree_bag = predict(object =model,   
                          newdata = model_test)  

# Compute the test MSE
mean((model_test$medv - tree_bag)^2)
```

#Modeling Random Forest in R with Caret
#how to model a ridge regression using the Caret package
#use the train method. We pass the same parameters as above, but in addition we pass the method = 'rf' model to tell Caret to use a lasso model.
```{r}
library(caret)

set.seed(1)
model <- train(
  medv ~ .,
  data = Boston,
  method = 'rf', localImp = TRUE
)
```

```{r}
explain_forest(model, interactions = TRUE)
```
model
#caret automatically trained over multiple hyper parameters. 
#plot those to visualize.
plot(model)
```

```{r}
forest_stats <-
measure_importance(model, measures = c("mse_increase",
"node_purity_increase",
"times_a_root"))
plot_importance_ggpairs(forest_stats)

```

```{r}
varImp
measure_importance(model)
importance_frame <- measure_importance(model)
```

```{r}
#install.packages("randomForestExplainer")
library(randomForestExplainer)
```

```{r}
load("min_depth_frame.rda")
min_depth_frame <- min_depth_distribution(model)


plot_min_depth_distribution(min_depth_frame, mean_sample = "relevant_trees", k = 15)
```

#Preprocessing with Caret
#center and scale our data
```{r}
preProcess = c("center", "scale")
set.seed(1)

model2 <- train(
  medv ~ .,
  data = Boston,
  method = 'rf',
  preProcess = c("center", "scale") #center and scale
)
model2
```

```{r}
set.seed(1)
#split and can check for overfitting
#return indexes that contains 80% of the data that we should use for training
inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]

#fit our model again using only the training data
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'rf',
  preProcess = c("center", "scale")
)
model3
```
#check our data on the test set
```{r}
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)

#calculate the RMSE and r2 to compare to the model above.
# RMSE
sqrt(mean((test.target - predictions)^2))

cor(test.target, predictions) ^ 2
```

#10-fold cross-validation
#common to use a data partitioning strategy like k-fold cross-validation that 
#resamples and splits our data many times.
#train the model on these samples and pick the best model
#easy with the trainControl method
```{r}
set.seed(1)
ctrl <- trainControl(
  method = "cv",
  number = 10, #for 10 fold
)

#retrain the model on these samples and pick the best model
set.seed(1)
model4 <- train(
  medv ~ .,
  data = training,
  method = 'rf',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4

plot(model4)
```

```{r}
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model4, newdata = test.features)

# RMSE
sqrt(mean((test.target - predictions)^2))

#improved our accuracy for our training data
# R2
cor(test.target, predictions) ^ 2
```

#Tuning Hyper Parameters
#o tune a random forest model, we can give the model different values of mtry. 
#Caret will retrain the model using different mtry and select the best version.
```{r}
set.seed(1)
tuneGrid <- expand.grid(
  mtry = c(2:8),
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'rf',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid,
  importance=TRUE
)
model5

plot(model5)
varImp(model5)
```
#second random forest run

#training Sample with 300 observations
```{r}
train=sample(1:nrow(Boston),300)
set.seed(1)
Boston.rf=randomForest(medv ~ . , data = Boston , subset = train)
Boston.rf

#Mean Squared Error and Variance explained are calculated using Out of Bag Error Estimation.

#Plotting the Error vs Number of Trees Graph.
plot(Boston.rf)
```
`
# Error and the Number of Trees.We can easily notice that how the Error is 
#dropping as we keep on adding more and more trees and average them.

#Random Forest model chose Randomly 4 variables to be considered at each split.
#now try all possible 13 predictors which can be found at each split.
#compare the Out of Bag Sample Errors and Error on Test set
```{r}
oob.err=double(13)
test.err=double(13)
#mtry is no of Variables randomly chosen at each split

for(mtry in 1:13) 
{
  rf=randomForest(medv ~ . , data = Boston , subset = train,mtry=mtry,ntree=400) 
  oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf,Boston[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(Boston[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}

#Test Error
test.err

#Out of Bag Error Estimation
oob.err
```

#Plotting both Test Error and Out of Bag Error
```{r}
matplot(1:mtry , cbind(oob.err,test.err), 
        pch=19 , 
        col=c("red","blue"),
        type="b",
        ylab="Mean Squared Error",
        xlab="Number of Predictors Considered at each Split")
        legend("topright",
        legend=c("Out of Bag Error","Test Error"),
        pch=19, col=c("red","blue"))
```
#four is the number of predictor variables that produced the smallest mean squared error
#for the fitted trees (Out of Bag error).

#the Red line is the Out of Bag Error Estimates and the Blue Line is the Error calculated on Test Set
#Error Tends to be minimized at around mtry=4 .
